# -*- coding: utf-8 -*-
"""cse546-hw3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yMxuCGVfc6nizk4r7_yEn0cjm_DKd6qq
"""

from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F

mnist = fetch_openml('mnist_784', cache=False)

X = mnist.data.astype('float32')
Y = mnist.target.astype('int64')
X /= 255.0

X_train = X[0:60000,:]
labels_train = Y[0:60000]
X_test = X[60000:,:]
labels_test = Y[60000:]

X_train = torch.from_numpy(X_train).float().cuda()
labels_train = torch.from_numpy(labels_train).long().cuda()
X_test = torch.from_numpy(X_test).float().cuda()
labels_test = torch.from_numpy(labels_test).long().cuda()

n, k, d, h = 60000, 10, 784, 64
n2 = 10000
step_size = .01

W0 = torch.empty(h,d,requires_grad=True,device=torch.device('cuda'))
W1 = torch.empty(k,h,requires_grad=True,device=torch.device('cuda'))
b0 = torch.empty(h,1,requires_grad=True,device=torch.device('cuda'))
b1 = torch.empty(k,1,requires_grad=True,device=torch.device('cuda'))

nn.init.uniform_(W0,-1/np.sqrt(h),1/np.sqrt(h))
nn.init.uniform_(b0,-1/np.sqrt(h),1/np.sqrt(h))
nn.init.uniform_(W1,-1/np.sqrt(k),1/np.sqrt(k))
nn.init.uniform_(b1,-1/np.sqrt(k),1/np.sqrt(k))

optimizer = torch.optim.Adam([W0,b0,W1,b1], lr=step_size)

def model(X,W0,b0,W1,b1,batch_size):
    temp = W0 @ torch.transpose(X,0,1)
    for i in range(batch_size):
        temp[:,i] = temp[:,i] + torch.transpose(b0,0,1)
    temp = W1 @ nn.functional.relu(temp)
    for i in range(batch_size):
        temp[:,i] = temp[:,i] + torch.transpose(b1,0,1)
    return torch.transpose(temp,0,1)

def pred_acc(W0,b0,W1,b1,X,Y,n):
    accuracy=n
    predictions = model(X,W0,b0,W1,b1,n)
    for i in range(n):
      if (torch.argmax(predictions[i,:]) != Y[i]):
        accuracy -= 1
    return accuracy/n

losses=[]

batch_size = 60000    #for stochastic gradient descent
accuracy=0
while (accuracy < .99):
    batch = np.random.randint(0,60000,size=batch_size)
    y_hat = model(X_train[batch,:],W0,b0,W1,b1,batch_size)
    target = labels_train[batch]
    loss = nn.functional.cross_entropy(y_hat,target)
    losses.append(loss.item())
    
    optimizer.zero_grad()
    loss.backward()
    
    optimizer.step()

    accuracy = pred_acc(W0,b0,W1,b1,X_train,labels_train,n)
    print(str(accuracy)) 
print('Finished!')

plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss vs Epoch')

print(pred_acc(W0,b0,W1,b1,X_test,labels_test,n2))