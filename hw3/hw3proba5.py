# -*- coding: utf-8 -*-
"""hw3probA5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O392OidrZmNJNmaWN3dScTQKnQnwWYeW
"""

import torch
import torchvision 
import torchvision.transforms as transforms
import torchvision.models as models
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
import numpy as np
import matplotlib.pyplot as plt

transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(), 
    transforms.Normalize(mean = [.485,.456,.406], std=[.229,.224,.225])]
)

train_data = torchvision.datasets.CIFAR10(root='cifar-10-batches.py', train=True, download=True, transform=transform)
train, val = random_split(train_data,[45000,5000])

trainloader = torch.utils.data.DataLoader(train, batch_size=1000, shuffle=True)
valloader = torch.utils.data.DataLoader(val, batch_size=1000, shuffle=True)

test_data = torchvision.datasets.CIFAR10(root='cifar-10-batches.py', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(test_data, batch_size=4, shuffle=False, num_workers=2)

model = torchvision.models.alexnet(pretrained=True)
for param in model.parameters():
    param.requires_grad=False
model.classifier[6] = nn.Linear(4096,10)
device = torch.device('cuda')
model.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
losses_train=list()
losses_val=list()
accuracies=list()

epochs = 20
for epoch in range(epochs):
    losses=list()
    for images, labels in trainloader:
        images = images.cuda()
        labels = labels.cuda()
        y_hat = model(images)
    
        loss_train = nn.functional.cross_entropy(y_hat,labels)

        losses.append(loss_train.item())
    
        model.zero_grad()
        loss_train.backward()
    
        optimizer.step()
    losses_train.append(torch.tensor(losses).mean())
    print(f'Epoch {epoch+1} training loss: {torch.tensor(losses).mean():.2f}')
    
    losses=list()
    correct=0
    for images, labels in valloader:
        images = images.cuda()
        labels = labels.cuda()
        with torch.no_grad():
            y_hat = model(images)
            _, predictions = torch.max(y_hat.data,1)
            correct += (predictions == labels).sum().item()
            
            loss_val = nn.functional.cross_entropy(y_hat,labels)
            losses.append(loss_val.item())
    accuracies.append(correct/5000)
    print(f'Epoch {epoch+1} validation accuracy : {correct/5000}')
    losses_val.append(torch.tensor(losses).mean())
    print(f'Epoch {epoch+1} validation loss: {torch.tensor(losses_val).mean():.2f}')

plt.plot(losses_train,label='Training Loss')
plt.plot(losses_val,label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss vs Epoch')

correct = 0
with torch.no_grad():
  for images, labels in testloader:
    images = images.cuda()
    labels = labels.cuda()
    y_hat = model(images)
    _, predictions = torch.max(y_hat.data,1)
    correct += (predictions == labels).sum().item()
print(f'Testing Accuracy: {correct/10000}')

model = torchvision.models.alexnet(pretrained=True)
#do not freeze layers
model.classifier[6] = nn.Linear(4096,10)
device = torch.device('cuda')
model.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
losses_train=list()
losses_val=list()
accuracies=list()

epochs = 20
for epoch in range(epochs):
    losses=list()
    for images, labels in trainloader:
        images = images.cuda()
        labels = labels.cuda()
        y_hat = model(images)
    
        loss_train = nn.functional.cross_entropy(y_hat,labels)

        losses.append(loss_train.item())
    
        model.zero_grad()
        loss_train.backward()
    
        optimizer.step()
    losses_train.append(torch.tensor(losses).mean())
    print(f'Epoch {epoch+1} training loss: {torch.tensor(losses).mean():.2f}')
    
    losses=list()
    correct=0
    for images, labels in valloader:
        images = images.cuda()
        labels = labels.cuda()
        with torch.no_grad():
            y_hat = model(images)
            _, predictions = torch.max(y_hat.data,1)
            correct += (predictions == labels).sum().item()
            
            loss_val = nn.functional.cross_entropy(y_hat,labels)
            losses.append(loss_val.item())
    accuracies.append(correct/5000)
    print(f'Epoch {epoch+1} validation accuracy : {correct/5000}')
    losses_val.append(torch.tensor(losses).mean())
    print(f'Epoch {epoch+1} validation loss: {torch.tensor(losses_val).mean():.2f}')

plt.plot(losses_train,label='Training Loss')
plt.plot(losses_val,label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss vs Epoch')

correct = 0
with torch.no_grad():
  for images, labels in testloader:
    images = images.cuda()
    labels = labels.cuda()
    y_hat = model(images)
    _, predictions = torch.max(y_hat.data,1)
    correct += (predictions == labels).sum().item()
print(f'Testing Accuracy: {correct/10000}')

test_loss = 0
counter = 0
with torch.no_grad():
  for images,labels in testloader:
    images = images.cuda()
    labels = labels.cuda()
    y_hat = model(images)
    test_loss += nn.functional.cross_entropy(y_hat,labels)
    counter += 1
print(f'Test Loss: {test_loss / counter}')

# A.6

import torch
import torchvision 
import torchvision.transforms as transforms
import torchvision.models as models
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
import numpy as np
import matplotlib.pyplot as plt

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

batch_size = 32

train_data = torchvision.datasets.CIFAR10(root='cifar-10-batches.py', train=True, download=True, transform=transform)
train, val = random_split(train_data,[45000,5000])

trainloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)
valloader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=True)

test_data = torchvision.datasets.CIFAR10(root='cifar-10-batches.py', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(test_data, batch_size=4, shuffle=False, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(3*32*32, 10)

    def forward(self, x):
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = self.fc(x)
        return x

criterion = nn.CrossEntropyLoss()

#trains model with given optimizer and step size
def train_model(optimizer_str,step_size,param1,param2,fig):
  #net = Net()  #for part (a)
  #net=Net2(param1) #for part (b)
  net=Net3(param1,param2)  #for part (c)
  net.to('cuda')
  acc_train = list()
  acc_val = list()

  if optimizer_str =='Adam':
    optimizer = torch.optim.Adam(net.parameters(),lr=step_size)
  if optimizer_str == 'SGD':
    optimizer = torch.optim.SGD(net.parameters(),lr=step_size,momentum=.09)
  for epoch in range(100):
    for i, data in enumerate(trainloader, 0):
      # get the inputs; data is a list of [inputs, labels]
      inputs, labels = data[0].to('cuda'), data[1].to('cuda')
      # zero the parameter gradients
      optimizer.zero_grad()
      # forward + backward + optimizer
      outputs = net(inputs)
      loss = criterion(outputs, labels)
      loss.backward()
      optimizer.step()

    correct = 0
    with torch.no_grad():
      for i, data in enumerate(trainloader,0):
        inputs, labels = data[0].to('cuda'), data[1].to('cuda')
        y_hat = net(inputs)
        _, predictions = torch.max(y_hat.data,1)
        correct += (predictions == labels).sum().item()
    print(f'''Learning Rate:{step_size},Optimizer:{optimizer_str},Epoch {epoch+1} 
      training accuracy = {correct/45000}''')
    acc_train.append(correct/45000)

    correct = 0
    with torch.no_grad():
      for i, data in enumerate(valloader,0):
        inputs, labels = data[0].to('cuda'), data[1].to('cuda')
        y_hat = net(inputs)
        _, predictions = torch.max(y_hat.data,1)
        correct += (predictions == labels).sum().item()
    print(f'''Learning Rate:{step_size},Optimizer:{optimizer_str},Epoch {epoch+1}
          validation accuracy = {correct/5000}''')
    acc_val.append(correct/5000)
    if (correct/5000)>0.75:
      break

  correct = 0
  with torch.no_grad():
    for i, data in enumerate(testloader,0):
      inputs, labels = data[0].to('cuda'), data[1].to('cuda')
      y_hat = net(inputs)
      _, predictions = torch.max(y_hat.data,1)
      correct += (predictions == labels).sum().item()
  print(f'Test Accuracy = {correct / 10000}')

  plt.figure(fig)
  plt.plot(acc_train,label='Training Accuracy')
  plt.plot(acc_val,label='Validation Accuracy')
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.ylim((0,1))
  plt.legend()
  plt.title(f'Learning Rate = {step_size}, Conv Size = {param1}, FCsize = {param2}')

steps = [1,.1,.01]
optim_strs = ['Adam','SGD']
fig=0
for step_size in steps:
  for optimizer_str in optim_strs:
    train_model(optimizer_str,step_size,fig)
    fig += 1

#train part (a) model on best hyperparameters
train_model('SGD',.01,1,0)

class Net2(nn.Module):
    def __init__(self, M):
        super().__init__()
        self.M = M
        self.fc1 = nn.Linear(3*32*32, self.M)
        self.fc2 = nn.Linear(self.M,10)

    def forward(self, x):
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = self.fc1(x)
        x = self.fc2(nn.functional.relu(x))
        return x

steps = [.01,.001]
optim_strs = ['Adam']
M_vals = [100,1000,10000]
fig=0
for step_size in steps:
  for M in M_vals:
    train_model(optimizer_str,step_size,M,fig)
    fig += 1

class Net3(nn.Module):
    def __init__(self,param1,param2):
        super().__init__()
        self.param1 = param1
        self.param2 = param2
        self.conv1 = nn.Conv2d(3, self.param1, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(self.param1, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, self.param2)
        self.fc2 = nn.Linear(self.param2, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(nn.functional.relu(self.conv1(x)))
        x = self.pool(nn.functional.relu(self.conv2(x)))
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = nn.functional.relu(self.fc1(x))
        x = nn.functional.relu(self.fc2(x))
        x = self.fc3(x)
        return x

#training part (b) network on best hyperparams
train_model('Adam',.001,1000,0)

#train model for part(c) and (d) 
param1_vals = [6, 50, 100, 250]
param2_vals = [120,500,1000]
step_sizes = [.01, .005, .001]
fig=0
for param1 in param1_vals:
  for param2 in param2_vals:
    for step_size in step_sizes:
      train_model('Adam',step_size,param1,param2,fig)
      fig += 1

train_model('Adam',.0005,50,500,0)